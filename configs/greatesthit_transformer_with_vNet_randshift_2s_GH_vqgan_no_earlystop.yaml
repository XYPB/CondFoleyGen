model:
  base_learning_rate: 5e-6
  target: specvqgan.models.av_cond_transformer.Net2NetTransformerAVCond
  params:
    cond_stage_key: feature
    clip: 50
    p_normalize: 1.0
    transformer_config:
      target: specvqgan.modules.transformer.mingpt.GPTFeats
      params:
        feat_embedding_config:
          target: torch.nn.Conv1d # conv is used for convenience of applying the same FC at each position (kernel_size=1, padding=0) â€“ donot change these params
          params:
            in_channels: 512  # feat_depth
            out_channels: 1024  # n_embd
            kernel_size: 1
            padding: 0
        GPT_config:
          vocab_size: 1024
          block_size: 160  # clip * 2 + how many frames (1)
          n_layer: 24
          n_head: 16
          n_embd: 1024
    first_stage_permuter_config:
      target: specvqgan.modules.transformer.permuter.ColumnMajor
      params:
        H: 5  # mel_num, num of feats in specs / down_factor
        W: 10  # cropped spec length / down_factor
    first_stage_config:
      target: specvqgan.models.vqgan.VQModel
      params:
        ckpt_path: 'logs/2022-09-11T23-55-13_greatesthit_codebook/checkpoints/last.ckpt' # e.g. 'logs/2022-09-11T23-55-13_greatesthit_codebook/checkpoints/last.ckpt'
        embed_dim: 256
        n_embed: 1024
        L: 2.0
        ddconfig:
          double_z: false
          z_channels: 256
          resolution: 160
          in_channels: 1
          out_ch: 1
          ch: 128
          ch_mult: [1, 1, 2, 2, 4]
          num_res_blocks: 2
          attn_resolutions: [10]
          dropout: 0.0
        lossconfig:
          target: specvqgan.modules.losses.DummyLoss
    # no permuter for the cond stage as the raw features is already a sequence
    cond_stage_config:
      target: specvqgan.modules.video_model.r2plus1d_18.r2plus1d18KeepTemp

lightning:
  modelcheckpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      monitor: val/loss
      mode: min
      save_last: True
      save_top_k: 3
  callbacks:
    image_logger:
      target: train.ImageLogger
      params:
        for_specs: True
        vocoder_cfg:
          target: train.VocoderMelGan
          params:
            ckpt_vocoder: './vocoder/logs/vggsound/'
  trainer:
    max_epochs: 50


data:
  target: train.ConditionedImageGreatestHitWaveModuleFromConfig
  params:
    batch_size: 8
    num_workers: 16

    wav_dir: 'data/greatesthit/greatesthit_processed'
    splits_path: 'data'
    random_crop: False
    mel_num: 80
    sample_rate: 22050
    spec_crop_len: 160
    L: 2.00
    rand_shift: True

    data_path: 'data/greatesthit/greatesthit_processed'

    train:
      target: specvqgan.data.greatesthit.CondGreatestHitWaveCondOnImageTrain
      params:
        dataset_cfg:
    validation:
      target: specvqgan.data.greatesthit.CondGreatestHitWaveCondOnImageValidation
      params:
        dataset_cfg:
    test:
      target: specvqgan.data.greatesthit.CondGreatestHitWaveCondOnImageTest
      params:
        dataset_cfg:
