<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-75863369-7');
var slideIndex = [1,1,1,1,1,1,1];
var slideId = ["mySlides0", "mySlides1","mySlides2","mySlides3","mySlides4","mySlides5","mySlides6"]
var dotId=["dot0","dot1",'dot2',"dot3","dot4","dot5","dot6"]
showSlides(1,0);
showSlides(1,1);
showSlides(1,2);
showSlides(1,3);
showSlides(1,4);
showSlides(1,5);
showSlides(1,6);
function plusSlides(n,i) {
    showSlides(slideIndex[i] += n,i);
}

function currentSlide(n,i) {
    showSlides(slideIndex[i] = n,i);
}

function showSlides(n,idx) {
    var i;
    var slides = document.getElementsByClassName(slideId[idx]);
    var dots = document.getElementsByClassName(dotId[idx]);
    if (n > slides.length) {slideIndex[idx] = 1}    
    if (n < 1) {slideIndex[idx] = slides.length}
    for (i = 0; i < slides.length; i++) {
        slides[i].style.display = "none";  
    }
    for (i = 0; i < dots.length; i++) {
        dots[i].className = dots[i].className.replace(" active", "");
    }
    slides[slideIndex[idx]-1].style.display = "block";  
    dots[slideIndex[idx]-1].className += " active";
}

</script>

<html>
<head>
    <title>Conditional Generation of Audio from Video via Foley Analogies</title>
    <!-- <meta property="og:image" content=images/teaser.png"/> Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="Conditional Generation of Audio from Video via Foley Analogies" />
    <meta property="og:description" content="Y. Du, Z. Chen, J. Salamon, B. Russell A. Owens." />
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- webpage template-->
    <link rel="stylesheet" href="assets/css/style.css">
    <!-- model-viewer css -->
    <link rel="stylesheet" href="assets/css/demo-style.css">

</head>

<body>
    <br>
    <!-- <center> -->
    <center>
        <span style="font-size:38px">Conditional Generation of Audio from Video via Foley Analogies</span>
    </center>
    <br><br>
    <table align=center width=80%>
        <tr>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://xypb.github.io/">Yuexi Du</a><sup>1,2</sup></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://ificl.github.io/">Ziyang Chen </a><sup>1</sup></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://www.justinsalamon.com/">Justin Salamon </a><sup>3</sup></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="https://bryanrussell.org/">Bryan Russell </a><sup>3</sup></span>
                </center>
            </td>
            <td align=center width=20%>
                <center>
                    <span style="font-size:20px"><a href="http://andrewowens.com/">Andrew Owens</a><sup>1</sup></span>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=60%>
        <tr>
            <td align=center width=33%>
                <center>
                    <span style="font-size:20px">University of Michigan<sup>1</sup></span>
                </center>
            </td>
            <td align=center width=33%>
                <center>
                    <span style="font-size:20px">Yale University<sup>2</sup></span>
                </center>
            </td>
            <td align=center width=33%>
                <center>
                    <span style="font-size:20px">Adobe Research<sup>3</sup></span>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=60%>
        <tr>
            <td align=center width=100%>
                <center>
                    <span style="font-size:20px">CVPR 2023</span>
                </center>
            </td>
        </tr>
    </table>
    <br>
    <table align=center width=60%>
        <tr>
            <td align=center width=50%>
                <center>
                    <span style="font-size:20px"><a href="TBD">[Paper]</a></span>
                </center>
            </td>
            <td align=center width=50%>
                <center>
                    <span style="font-size:20px"><a href="https://github.com/XYPB/CondFoleyGen">[Github]</a></span>
                </center>
            </td>
        </tr>
    </table>

    <br>

    <table align=center width=80%>
        <tr>
            <td width=100%>
                <center>
                    <!-- <img src="images/teaser_wild_compressed.png" width="100%"></img><br> -->
                    <video width="1000" controls>
                        <source src="images/teaser_video_compressed.mp4" type="video/mp4" />
                    </video>
                    <br>
                </center>
            </td>
        </tr>
        <tr>
        <td width=95%>
            <center>
                <span style="font-size:16px"><i> 
                    We generate a soundtrack for a silent input video, given a user-provided conditional example specifying what its audio should "sound like."</i>
            </center>
        </td>
        </tr>
    </table>
    <br>
    <!-- <hr> -->

    <table align=center width=80%>
        <center><h1>Abstract</h1></center>
        <tr>
            <td class="my-paragraph">
                The sound effects that designers add to videos are designed to convey a particular artistic effect and, thus, may be quite different from a scene's true sound. Inspired by the challenges of creating a soundtrack for a video that differs from its true sound, but that nonetheless matches the actions occurring on screen, we propose the problem of <i><b>conditional Foley</b></i>. We present the following contributions to address this problem. First, we propose a pretext task for training our model to predict sound for an input video clip using a conditional audio-visual clip sampled from another time within the same source video. Second, we propose a model for generating a soundtrack for a silent input video, given a user-supplied example that specifies what the video should "sound like". We show through human studies and automated evaluation metrics that our model successfully generates sound from video, while varying its output according to the content of a supplied example.
            </td>
        </tr>
    </table>
    <br><br>
    <hr>

    <table align=center width=100%>
        <center>
            <h1>Video Demo</h1>
        </center>
        <div>
            <p align="center"><i>*Please wear an earphone or headset and turn up the volume slightly for the best quality.</i></p>
            <p align="center">
            <iframe style="width: 960px; height: 540px; min-width:960px; min-height:540px;" src="https://www.youtube.com/embed/MrXUmoHqwDc" frameborder="0"
                allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </p>
        </div>
    </table>
    <br><br>
    <hr>

    <table align=center width=80%>
        <center><h1>Learning from Foley Analogies</h1></center>
        <tr>
            <td class="my-paragraph">
                We propose a self-supervised pretext task for learning conditional Foley. Our pretext task exploits the fact that natural videos tend to contain repeated events that produce closely related sounds. During training, we randomly sample two pairs of audio-visual slips from a video, and the use one as the conditional example for the other. Our model then learns to infer the types of actions within the sene from the conditional example, and to generate analogous sounds to match the input example.
            </td>
        </tr>
        <tr>
            <td width=100%>
                <center> 
                    <video width="1000" autoplay loop muted playsinline id="method">
                        <source src="images/method_video_crop.mp4" type="video/mp4" />
                    </video>
                    <script>
                        document.getElementById("method").playbackRate = 1.0;
                    </script>
                    <br>
                </center>
            </td>
        </tr>
        <tr>
            <td class="my-paragraph">
                Our model takes the silent input video and the conditional audio-visual example as input. It then autoregressively generates a spectrogram from an input videos using a VQ-GAN, then converts the sound to a waveform.
            </td>
        </tr>
    </table>
    <br><br>

    <table align=center width=80%>
        <center><h1>Inference-time Audio Re-ranking</h1></center>
        <tr>
            <td width=100%>
                <center> 
                    <video width="1000" autoplay loop muted playsinline id="method">
                        <source src="images/ranking_video_crop.mp4" type="video/mp4" />
                    </video>
                    <script>
                        document.getElementById("method").playbackRate = 1.0;
                    </script>
                    <br>
                </center>
            </td>
        </tr>
        <tr>
            <td width=100% class="my-paragraph">
                inspired by other work in cross-model generation, we use <i>re-ranking</i> to improve our model's predictions. We generate a large number of sounds, the select the best one, as judged by a separate classifier. Instead of using a classifier that judges the multimodal agreement between the input and output, we propose to use an off-the-shelf audio-visual synchronization model to measure the temporal alignment between the predicted sound and the input video.
            </td>
        </tr>
    </table>
    <br><br>

    <hr>

    <table align=center width=100%>
        <center>
            <h1>Example Results</h1>
        </center>

        <center>
            <h2><i>Internet</i> Videos</h2>
            <i style="font-size: 15px;">*generated with model trained on the CountixAV dataset</i>
            <br><br>
        </center>
        <center> 
            <video width="750" controls width="750">
                <source src="generated_videos/example_videos/example_video1_compress.mp4" type="video/mp4" />
            </video>
            <br>
        </center>
        <br><br>

        <center>
            <h2><i>Greatest Hits</i> Dataset Results</h2>
        </center>
        <center> 
            <video width="750" controls width="750">
                <source src="generated_videos/example_videos/example_video4_compress.mp4" type="video/mp4" />
            </video>
            <br>
        </center>
        <br><br>
    <hr>

    <table align=center width=100%>
        <center>
            <h1>Generation with Different Conditions</h1>
        </center>

        <th width="50%" text-align="center">
            <center>
                <h2><i>Internet</i> Videos</h2>
                <i style="font-size: 15px;">*generated with model trained on the CountixAV dataset</i>
                <!-- <br><br> -->
            </center>
            <center> 
                <video width="100%" controls width="100%">
                    <source src="generated_videos/example_videos/example_video5_compress.mp4" type="video/mp4" />
                </video>
                <br>
            </center>
            <br><br>
        </th>
        <th><b font-size="100">&nbsp;:&nbsp;</b></th>
        <th width="50%" text-align="center">
            <center>
                <h2><i>Greatest Hits</i> Dataset Results</h2>
                <i style="font-size: 15px;">&nbsp;</i>
                <!-- <br><br> -->
            </center>
            <center> 
                <video width="100%" controls width="100%">
                    <source src="generated_videos/example_videos/example_video6_compress.mp4" type="video/mp4" />
                </video>
                <br>
            </center>
            <br><br>
        </th>

    </table>
    <br><br>
    <hr>


    <table align=center width=60%>
        <center><h1>Paper and Supplementary Material</h1></center>
        <tr>
            <td><a href=""><img class="layered-paper-big" style="height:175px" src="./images/paper.png"/></a></td>
            <td><span style="font-size:14pt">Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, Andrew Owens.<br>
                <b>Conditional Generation of Audio from Video via Foley Analogies.</b><br>
                CVPR 2023.<br>
                <!-- (hosted on <a href="#">ArXiv</a>)<br> -->
                (<a href="TBD">Arxiv</a>)<br>
                <span style="font-size:4pt"><a href=""><br></a>
                </span>
            </td>
        </tr>
    </table>
    <br>

    <table align=center width=60%>
        <tr>
            <td><span style="font-size:14pt"><center>
                <a href="./docs/bibtex.txt">[Bibtex]</a>
            </center></td>
        </tr>
    </table>

    <hr>
    <br>

    <table align=center width=80%>
        <tr>
            <td width=80%>
                <left>
                    <center><h1>Acknowledgements</h1></center>
                    We thank Jon Gillick, Daniel Geng, and Chao Feng for the helpful discussions. Our code base is developed upon two amazing projects proposed by Vladimir Iashin <i>et.al,</i>, check out those projects here ([<a href="https://iashin.ai/SpecVQGAN">SpecVQGAN</a>], [<a href="https://iashin.ai/SparseSync">SparseSync</a>]). This work was funded in part by DARPA Semafor and Cisco Systems, and by a gift from Adobe. The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. We credit the licensed videos that appears in the webpage and the demo video <a href="./docs/video_credit.txt">here</a> The webpage template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">Colorization</a> project. 
                </left>
            </td>
        </tr>
    </table>

<br>
</body>
</html>