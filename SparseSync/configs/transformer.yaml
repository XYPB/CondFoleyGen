action: train_avsync_model

model:
  target: model.sync_model.AVSyncModel
  params:
    afeat_extractor:
      is_trainable: True
      target: model.modules.feature_extractors.ResNet18AudioFeatures
      params:
        # ckpt_path: null
        ckpt_path: ./logs/feature_extractors/22-06-24T08-10-33/ResNetAudio-22-06-24T08-10-33.pt  # 5s 16000hz
    vfeat_extractor:
      is_trainable: True
      target: model.modules.feature_extractors.S3DVisualFeatures
      params:
        # ckpt_path: null
        ckpt_path: ./model/modules/feat_extractors/visual/S3D_kinetics400_torchified.pt
    a_bridge_cfg:
      target: model.modules.bridges.DoNothingBridge
    v_bridge_cfg:
      target: model.modules.bridges.ConvBridgeVisual
      params:
        in_channels: 1024 # 1024 for s3d, 384 for vit_small, 512 for RN18
        out_channels: ${model.params.transformer.params.n_embd}
        kernel_size: [1, 1, 1]
        stride: [1, 1, 1]
        padding: [0, 0, 0]
        bias: True
    transformer:
      target: model.modules.transformer.Transformer
      params:
        num_offset_cls: 21  # e.g. 21 for (-2.0, -1.8, ..., 0.0, ..., 1.8, 2.0)
        visual_block_shape: [16, 7, 7] # see the output of the feature_extractors [50, 7, 7] - rn; [16, 7, 7] - s3d; [50, 14, 14] - vit
        audio_block_shape: [9, 20] # see the output of the feature_extractors
        pre_norm_cfg:
          target: torch.nn.LayerNorm  # LayerNorm or Identity (no normalization) model.modules.transformer.L2Normalize
          params:
            normalized_shape: ${model.params.transformer.params.n_embd}
        n_layer: 3
        n_head: 8
        n_embd: 512
        tok_pdrop: 0.0
        embd_pdrop: 0.1
        resid_pdrop: 0.1
        attn_pdrop: 0.1
        vis_pos_emb_module:
          target: model.modules.transformer.PositionEmbeddingLearnedVisual
          params:
            block_shape: ${model.params.transformer.params.visual_block_shape}
            n_embd: ${model.params.transformer.params.n_embd}
        aud_pos_emb_module:
          target: model.modules.transformer.PositionEmbeddingLearnedAudio
          params:
            block_shape: ${model.params.transformer.params.audio_block_shape}
            n_embd: ${model.params.transformer.params.n_embd}

training:
  base_learning_rate: 5e-6
  base_batch_size: 10
  num_workers: 8
  num_epochs: 10000
  patience: 140
  to_max_metric: True
  metric_name: 'accuracy_1'
  early_stop_phase: 'valid'  # care about which phase when deciding to early stop
  use_half_precision: True
  mixup_alpha: 0.0 # the distribution parameter. Use 0.0 to turn off mixup
  seed: 1337
  run_test_only: False
  resume: False
  finetune: False
  detect_anomaly: False
  dist_backend: 'nccl'
  max_clip_norm: 1
  run_corrupted_val: False
  lr_scheduler:
    name: 'constant_with_warmup'  # 'constant' 'constant_with_warmup'
    warmup: 1000 # iterations to recover from base_learning_rate / 100
  optimizer:
    name: adam # adamw, adam or sgd
    betas: [0.9, 0.999]
    momentum: 0.9
    weight_decay: 0

data:
  crop_len_sec: 5
  max_off_sec: 2
  vids_path: 'PLACEHOLDER' # something that ends with 'CODEC_video_XXfps_YYYside_ZZZZZhz' or '..._25fps_...'
  size_before_crop: 256  # video resolution -> size_before_crop resolution -> input_size (crop resolution)
  input_size: 224
  do_offset: True
  p_color_jitter: 0.0  # ignored if 0 # ignored if 0
  p_gray_scale: 0.0  # ignored if 0
  sometimes_upscale_p: 0.0  # how often to apply the smaller crop and upscale? if 0.0 or null, works as RGBSpatialCrop
  is_spatial_crop_random: True  # if the crop transform should be random or just center crop should be used
  audio_jitter_sec: 0.05
  p_horizontal_flip: 0.5
  p_audio_aug: 0.0
  to_freeze_frames: False
  to_corrupt_audio: False
  corrupt_type: 'mute'  # 'mute' 'rand'
  active_min_overlap_sec: 1
  # changing `dataset` arguments here won't affect the init call. See train_utils.get_datasets
  dataset:
    target: dataset.vggsound.VGGSoundSparsePicked
    params:
      load_fixed_offsets_on_test: True
      vis_load_backend: 'read_video'
      size_ratio: null  # null or 1.0: full dataset; a ratio will use a proportion of it

# sequentially defined
transform_sequence_train:
  - target: dataset.transforms.EqualifyFromRight
    params:
      clip_max_len_sec: 10 # for LRS3 this can be increased to allow more training data as clips may be >10s
  - target: dataset.transforms.RGBSpatialCropSometimesUpscale
    params:
      sometimes_p: ${data.sometimes_upscale_p}
      smaller_input_size: 192 # the size of the smaller crop. null 192 112
      target_input_size: ${data.input_size}
      is_random: ${data.is_spatial_crop_random}
  - target: dataset.transforms.TemporalCropAndOffsetRandomFeasible
    params:
      crop_len_sec: ${data.crop_len_sec}
      max_off_sec: ${data.max_off_sec}
      max_wiggle_sec: ${data.audio_jitter_sec}
      do_offset: ${data.do_offset}
      grid_type: 'linspace' # uniform linspace
      grid_size: ${model.params.transformer.params.num_offset_cls}
  - target: dataset.transforms.RandomApplyColorDistortion
    params:
      p_color_jitter: ${data.p_color_jitter}
      s: 1.0 # strength of the color jitter if applied
      p_gray_scale: ${data.p_gray_scale}
  - target: dataset.transforms.RandomHorizontalFlip
    params:
      p: ${data.p_horizontal_flip}
  - target: dataset.transforms.FreezeFrames
    params:
      max_off_sec: ${data.max_off_sec}
      active_min_overlap_sec: ${data.active_min_overlap_sec}
      to_freeze_frames: ${data.to_freeze_frames}
  - target: dataset.transforms.CorruptAudio
    params:
      max_off_sec: ${data.max_off_sec}
      active_min_overlap_sec: ${data.active_min_overlap_sec}
      to_corrupt_audio: ${data.to_corrupt_audio}
      corrupt_type: ${data.corrupt_type}
  - target: dataset.transforms.RGBToFloatToZeroOne
  - target: dataset.transforms.RGBNormalize
    params:
      mean: [0.485, 0.456, 0.406] # typical torchvision normalization values
      std: [0.229, 0.224, 0.225]
  - target: dataset.transforms.AudioRandomReverb
    params:
      p: ${data.p_audio_aug}
  - target: dataset.transforms.AudioRandomVolume
    params:
      p: ${data.p_audio_aug}
      gain: 2.0
      gain_type: 'amplitude'
  - target: dataset.transforms.AudioRandomPitchShift
    params:
      p: ${data.p_audio_aug}
      shift: 1000
  - target: dataset.transforms.AudioRandomLowpassFilter
    params:
      p: ${data.p_audio_aug}
      cutoff_freq: 100
  - target: dataset.transforms.AudioRandomGaussNoise
    params:
      p: ${data.p_audio_aug}
      amplitude: 0.01
  - target: dataset.transforms.AudioSpectrogram
    params:
      n_fft: 512
      hop_length: 128  # n_fft // 4
  - target: dataset.transforms.AudioLog
  - target: dataset.transforms.AudioRandomFreqMask
    params:
      p: ${data.p_audio_aug}
      freq_mask_param: 64
  - target: dataset.transforms.AudioRandomTimeMask
    params:
      p: ${data.p_audio_aug}
      time_mask_param: 200
  - target: dataset.transforms.AudioStandardNormalize
  - target: dataset.transforms.AudioUnsqueezeChannelDim
    params:
      dim: 0

transform_sequence_test:
  - target: dataset.transforms.EqualifyFromRight
  - target: dataset.transforms.RGBSpatialCrop
    params:
      input_size: ${data.input_size}
      is_random: False
  - target: dataset.transforms.TemporalCropAndOffsetRandomFeasible
    params:
      crop_len_sec: ${data.crop_len_sec}
      max_off_sec: ${data.max_off_sec}
      do_offset: ${data.do_offset}
      grid_type: 'linspace' # uniform linspace
      grid_size: ${model.params.transformer.params.num_offset_cls}
  - target: dataset.transforms.FreezeFrames
    params:
      max_off_sec: ${data.max_off_sec}
      active_min_overlap_sec: ${data.active_min_overlap_sec}
      to_freeze_frames: ${data.to_freeze_frames}
  - target: dataset.transforms.CorruptAudio
    params:
      max_off_sec: ${data.max_off_sec}
      active_min_overlap_sec: ${data.active_min_overlap_sec}
      to_corrupt_audio: ${data.to_corrupt_audio}
      corrupt_type: ${data.corrupt_type}
  - target: dataset.transforms.RGBToFloatToZeroOne
  - target: dataset.transforms.RGBNormalize
    params:
      mean: [0.485, 0.456, 0.406] # typical torchvision normalization values
      std: [0.229, 0.224, 0.225]
  - target: dataset.transforms.AudioSpectrogram
    params:
      n_fft: 512
      hop_length: 128  # n_fft // 4
  - target: dataset.transforms.AudioLog
  - target: dataset.transforms.AudioStandardNormalize
  - target: dataset.transforms.AudioUnsqueezeChannelDim
    params:
      dim: 0

logging:
  logdir: './logs/sync_models'
  log_code_state: True
  # patterns to ignore when backing up the code folder
  patterns_to_ignore: ['logs', '.git', '__pycache__', 'data', '*.pt', 'sbatch_logs', '*.mp4', '*.wav', '*.jpg', '*.gif']
  use_wandb: True
